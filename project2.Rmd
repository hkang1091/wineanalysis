---
title: "Project2"
author: "HanGyu Kang"
date: "April 20, 2019"
output:
  pdf_document: default
  word_document: default
---

```{r}
dat <- read.delim('wine.data', header=F, sep = ',')
colnames(dat)<- c("Class","Alcohol","Malicacid","Ash","AlcalinityofAsh","Magnesium","Totalphenols","Flavanoids","Nonflavanoidphenols", "Proanthocyanins","Colorintensity", "Hue", "OD280OD315ofdilutedwines", "Proline")
dat$Class <- as.factor(dat$Class)

set.seed(4052)
train.ind <- sample(1:nrow(dat),0.6*nrow(dat))
train.dat <- dat[train.ind,]
test.dat <- dat[-train.ind,]
```

```{r}
library(gbm)
pow <- seq(-10, 0, by = 0.1)
lambdas <- 10^pow
test.boosting.error <- rep(NA, length(lambdas))
for (i in 1:length(lambdas)) {
  set.seed(4052)
  boost.dat <- gbm(Class~., data = train.dat, n.trees=1000, interaction.depth = 1, distribution = "multinomial", shrinkage = lambdas[i])
  test.pred <- apply(predict(boost.dat, test.dat, n.trees=1000), 1, which.max)
  test.boosting.error[i] <- 1-sum(diag(table(test.pred, test.dat$Class)))/sum(table(test.pred, test.dat$Class))
}
plot(lambdas, test.boosting.error, type="b", xlab = "lambda", ylab="Classification Error Rate")

min.lambda <- lambdas[which.min(train.boosting.error)]
min.lambda

test.boosting.error1 <- rep(NA, 1000)
for (i in 1:1000) {
  set.seed(4052)
  boost.dat <- gbm(Class~., data = train.dat, n.trees=i, interaction.depth = 1, distribution = "multinomial", shrinkage = min.lambda)
  test.pred <- apply(predict(boost.dat, test.dat, n.trees=i), 1, which.max)
  test.boosting.error1[i] <- 1-sum(diag(table(test.pred, test.dat$Class)))/sum(table(test.pred, test.dat$Class))
}

test.boosting.error2 <- rep(NA, 1000)
for (i in 1:1000) {
  set.seed(4052)
  boost.dat <- gbm(Class~., data = train.dat, n.trees=i, interaction.depth = 2, distribution = "multinomial", shrinkage = min.lambda)
  test.pred <- apply(predict(boost.dat, test.dat, n.trees=i), 1, which.max)
  test.boosting.error2[i] <- 1-sum(diag(table(test.pred, test.dat$Class)))/sum(table(test.pred, test.dat$Class))
}

library(randomForest)
#randomforest
test.rf.error <- rep(NA, length=1000)
for(i in 1:1000){
  set.seed(4052)
  rf.dat = randomForest(Class~., data=train.dat, mtry = floor(sqrt(13)), ntree=i)
  yhat.rf = predict(rf.dat, newdata = test.dat)
  test.rf.error[i] <- 1-(sum(diag(table(yhat.rf, test.dat$Class))))/sum(table(yhat.rf, test.dat$Class))
  }

#bagging
test.bag.error <- rep(NA, length=1000)
for(i in 1:1000){
  set.seed(4052)
  bag.dat = randomForest(Class~., data=train.dat, mtry = 13, ntree=i)
  yhat.bag = predict(bag.dat, newdata = test.dat)
  test.bag.error[i] <- 1-(sum(diag(table(yhat.bag, test.dat$Class))))/sum(table(yhat.bag, test.dat$Class))
  }

ntree <- 1:1000
plot(ntree, test.rf.error, type = 'l', ylab="Classification Error Rate", ylim = c(0, 0.20), col=2, lwd = 2)
par(new=T)
plot(ntree, test.bag.error, type = 'l', ylab="Classification Error Rate", ylim = c(0, 0.20), col=4)
par(new=T)
plot(ntree, test.boosting.error1, type = 'l', ylab="Classification Error Rate", ylim = c(0, 0.20), col=3)
par(new=T)
plot(ntree, test.boosting.error2, type = 'l', ylab="Classification Error Rate", ylim = c(0, 0.20), col=5)
legend("topright", c("RandomForest","Bagging","Boosting: depth=1", "Boosting: depth=2"), lwd=c(1,1), col=c(2,4,3,5))


#randomforest
set.seed(4052)
rf.dat = randomForest(Class~., data=train.dat, mtry = floor(sqrt(13)), ntree=1000)

yhat.rf = predict(rf.dat, newdata = test.dat)
table(yhat.rf, test.dat$Class)
1-mean(yhat.rf == test.dat$Class)

importance(rf.dat)
varImpPlot(rf.dat)

#boosting
set.seed(4052)
boost.dat <- gbm(Class ~ ., data = train.dat, n.trees = 1000, interaction.depth = 2, distribution = "multinomial", shrinkage = min.lambda)
yhat.bt <- apply(predict(boost.dat, test.dat, n.trees=1000), 1, which.max)
table(yhat.bt, test.dat$Class)
1-mean(yhat.bt == test.dat$Class)
summary(boost.dat)
```